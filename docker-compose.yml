version: '3.2'

services:
  namenode:
    image: esitull/stack-hadoop-docker:namenode
    hostname: namenode
    networks:
      - hadoop2
    volumes:
      - namenode1:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=cluster1
    env_file:
      - ./hadoop.env
    ports:
      - 50070:50070
    logging:
      options:
        max-size: 50m
    deploy:
       mode: replicated
       replicas: 1
       restart_policy:
         condition: on-failure
  datanode:
    image: esitull/stack-hadoop-docker:datanode
    networks: 
      - hadoop2
    volumes:
      - datanode1:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    env_file:
      - ./hadoop.env
    deploy:
      mode: global
      restart_policy:
        condition: on-failure

  resourcemanager:
    image: esitull/stack-hadoop-docker:resourcemanager
    hostname: resourcemanager
##    command: tail -f /var/log/dmesg
    networks: 
      - hadoop2
    env_file:
      - ./hadoop.env
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      #- YARN_CONF_yarn_log___aggregation___enable=true
    ports:
      - target: 8030
        published: 8030
        mode: host
      - target: 8032
        published: 8032
        mode: host
      - target: 8088
        published: 8088
        mode: host

    #    ports:
    #      - 8030:8030
    #      - 8032:8032
    #      - 8088:8088
    logging:
      options:
        max-size: 50m
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
         condition: on-failure
  nodemanager:
    image: esitull/stack-hadoop-docker:nodemanager
    depends_on:
      - resourcemanager
    networks: 
      - hadoop2
    env_file:
      - ./hadoop.env
    ports:
      - 8042:8042 
    logging:
      options:
        max-size: 50m
    deploy:
       mode: global 
       #replicas: 6  
       restart_policy:
         condition: on-failure

  spark:
    image: esitull/stack-hadoop-docker:spark
    hostname: spark
    volumes:
      - datosspark:/datos
    networks: 
       - hadoop2
    env_file:
      - ./hadoop.env
    #environment:
    #- CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    # - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    ports:
      - "2222:22"
    logging:
      options:
        max-size: 50m
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
    command: tail -f /var/log/dmesg
  gatk:
    image: esitull/stack-hadoop-docker:gatk
    hostname: gatk
    volumes:
      - datosspark:/datos
    networks: 
       - hadoop2
    env_file:
      - ./hadoop.env
    #environment:
    #- CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    # - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    ports:
      - "2223:22"
    logging:
      options:
        max-size: 50m
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
    command: tail -f /var/log/dmesg


volumes:
  namenode1:
  datanode1:
  datosspark:

networks:
  hadoop2:
